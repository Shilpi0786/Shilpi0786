[
  {
    "objectID": "Appendix.html",
    "href": "Appendix.html",
    "title": "Appendix",
    "section": "",
    "text": "Name - Shilpi Kumari University of Canada West 11 May , 2025"
  },
  {
    "objectID": "Appendix.html#appendix",
    "href": "Appendix.html#appendix",
    "title": "Appendix",
    "section": "Appendix",
    "text": "Appendix"
  },
  {
    "objectID": "Appendix.html#portfolio-outline---references",
    "href": "Appendix.html#portfolio-outline---references",
    "title": "Appendix",
    "section": "Portfolio Outline - References",
    "text": "Portfolio Outline - References\n\nhttps://ankitpakhale.netlify.app/portfolio\nhttps://a-coderr.github.io/portfolio-website/#home"
  },
  {
    "objectID": "Appendix.html#annoated-llm-response-comment",
    "href": "Appendix.html#annoated-llm-response-comment",
    "title": "Appendix",
    "section": "Annoated LLM Response Comment",
    "text": "Annoated LLM Response Comment\nI have ensured authenticity and relevance in the annotation process. I carefully reviewed the LLM outline, got the basic idea behind the concept, and annotated it based on personalization, skills, projects, aligning it with my actual academic and professional experience. I have used drafts as structured starting points and made generic revisions-\npersonalization - I have updated my professional experience and added real-time projects I have been working on to highlight my skills and experiences. I have listed projects such as AWS static site deployments, customer churn analysis, and work on under academic coursework.I have added other business projects, such as POpIN and an unsweetened organic marketing project, to highlight my skills on real-time projects. I have worked with different clients.\nContextual Editing - Some of the LLM phrases were edited for clarity and professionalism to simplify my tone. I have tried to edit in formal and more authentic, engaging phrases.\nVisual and Structural Changes - I have added sections like skills with different highlights of my technical and soft skills.I have removed unauthentic certifications listed in examples and only placed the authentic certifications completed by me.I have also included a logo of my own design created using Canva for maki ng it look visually appealing. Also, I have styled the CSS page using the styling formatting and dark-themed style for a bold look.\nAccuracy Checks- I have only included the experience, skills or certification which I hold and haven‚Äôt included LLM-generated inputs."
  },
  {
    "objectID": "Appendix.html#please-find-the-link-to-the-portfolio-posted-on-github-below--",
    "href": "Appendix.html#please-find-the-link-to-the-portfolio-posted-on-github-below--",
    "title": "Appendix",
    "section": "Please find the link to the portfolio posted on GitHub below -",
    "text": "Please find the link to the portfolio posted on GitHub below -\n\nhttps://shilpi0786.github.io/Shilpi0786/"
  },
  {
    "objectID": "Appendix.html#new-draft-section-outlines-llm-generated",
    "href": "Appendix.html#new-draft-section-outlines-llm-generated",
    "title": "Appendix",
    "section": "üìÑ New Draft Section Outlines (LLM-Generated)",
    "text": "üìÑ New Draft Section Outlines (LLM-Generated)\n\nHome / About Me (index.qmd) markdown Copy Edit ‚Äî title: ‚ÄúAbout Me‚Äù format: html ‚Äî"
  },
  {
    "objectID": "Appendix.html#career-objectives",
    "href": "Appendix.html#career-objectives",
    "title": "Appendix",
    "section": "üéØ Career Objectives",
    "text": "üéØ Career Objectives\n\nLeverage technology to enhance event collaboration and customer engagement.\nDrive marketing strategy through data-driven insights.\nFoster inclusive leadership in international business contexts."
  },
  {
    "objectID": "Appendix.html#quick-facts",
    "href": "Appendix.html#quick-facts",
    "title": "Appendix",
    "section": "üì∏ Quick Facts",
    "text": "üì∏ Quick Facts\n\nüìç Based in Vancouver, Canada\n‚òï Avid cook & cat enthusiast\nüìö Passionate about business innovation\n\n\nResume / CV (resume.qmd) markdown Copy Edit ‚Äî title: ‚ÄúResume‚Äù format: html ‚Äî"
  },
  {
    "objectID": "Appendix.html#education",
    "href": "Appendix.html#education",
    "title": "Appendix",
    "section": "üéì Education",
    "text": "üéì Education\nUniversity Canada West\nMaster of Business Administration\nExpected: 2025"
  },
  {
    "objectID": "Appendix.html#experience",
    "href": "Appendix.html#experience",
    "title": "Appendix",
    "section": "üßë‚Äçüíº Experience",
    "text": "üßë‚Äçüíº Experience\nProject Assistant, [Company Name], 2023\n- Coordinated interdepartmental communications\n- Supported process optimization efforts\nPopIn ‚Äì Capstone Project\n- Designed host engagement strategy using SWOT & PESTELE frameworks\n- Recommended marketing and data-driven growth initiatives"
  },
  {
    "objectID": "Appendix.html#skills",
    "href": "Appendix.html#skills",
    "title": "Appendix",
    "section": "üõ†Ô∏è Skills",
    "text": "üõ†Ô∏è Skills\n\nProject Management\n\nMarket Research\n\nStrategic Analysis (SWOT, PESTEL)\n\nCross-Cultural Communication"
  },
  {
    "objectID": "Appendix.html#certifications",
    "href": "Appendix.html#certifications",
    "title": "Appendix",
    "section": "üìú Certifications",
    "text": "üìú Certifications\n\nGoogle Data Analytics (Coursera)\n\nHubSpot Inbound Marketing\n\n\nProjects & Case Studies (projects.qmd) markdown Copy Edit ‚Äî title: ‚ÄúProjects & Case Studies‚Äù format: html ‚Äî"
  },
  {
    "objectID": "Appendix.html#sustainable-fashion-market-entry",
    "href": "Appendix.html#sustainable-fashion-market-entry",
    "title": "Appendix",
    "section": "üõçÔ∏è Sustainable Fashion Market Entry",
    "text": "üõçÔ∏è Sustainable Fashion Market Entry\nType: Strategy Simulation | Role: Consultant | Date: Winter 2025\n- Conducted PESTEL & competitor analysis\n- Recommended entry via e-commerce & partnerships\n- Created phased rollout with digital marketing focus"
  },
  {
    "objectID": "Appendix.html#subscription-app-retention-analytics",
    "href": "Appendix.html#subscription-app-retention-analytics",
    "title": "Appendix",
    "section": "üìä Subscription App Retention Analytics",
    "text": "üìä Subscription App Retention Analytics\nType: Data Analytics Project | Role: Lead Analyst | Date: Fall 2024\n- Analyzed churn & retention trends in Power BI\n- Identified UX & pricing improvement opportunities\n- Created visual dashboard and insights report"
  },
  {
    "objectID": "Appendix.html#organizational-diagnosis-mba-course",
    "href": "Appendix.html#organizational-diagnosis-mba-course",
    "title": "Appendix",
    "section": "üß© Organizational Diagnosis (MBA Course)",
    "text": "üß© Organizational Diagnosis (MBA Course)\nPerformed a detailed internal/external environment analysis: - Applied academic frameworks (APA citations included)\n- Made strategic recommendations to support scalability\n\nSkills & Certifications (skills.qmd) markdown Copy Edit ‚Äî title: ‚ÄúSkills & Certifications‚Äù format: html ‚Äî"
  },
  {
    "objectID": "Appendix.html#business-strategy",
    "href": "Appendix.html#business-strategy",
    "title": "Appendix",
    "section": "üß© Business Strategy",
    "text": "üß© Business Strategy\n\nSWOT, PESTEL, Porter‚Äôs 5 Forces\n\nOrganizational Diagnosis & Market Entry Plans"
  },
  {
    "objectID": "Appendix.html#marketing-analytics",
    "href": "Appendix.html#marketing-analytics",
    "title": "Appendix",
    "section": "üìà Marketing & Analytics",
    "text": "üìà Marketing & Analytics\n\nCustomer Journey Mapping\n\nPower BI, Excel, Google Analytics\n\nContent & Campaign Strategy"
  },
  {
    "objectID": "Appendix.html#communication",
    "href": "Appendix.html#communication",
    "title": "Appendix",
    "section": "üó£Ô∏è Communication",
    "text": "üó£Ô∏è Communication\n\nProfessional Writing & Presentations\n\nTeam Collaboration & Coordination\n\nCross-Cultural Communication"
  },
  {
    "objectID": "Appendix.html#email",
    "href": "Appendix.html#email",
    "title": "Appendix",
    "section": "üìß Email",
    "text": "üìß Email\nmengzhe.dai@myucwest.ca"
  },
  {
    "objectID": "Appendix.html#phone-whatsapp",
    "href": "Appendix.html#phone-whatsapp",
    "title": "Appendix",
    "section": "üì± Phone & WhatsApp",
    "text": "üì± Phone & WhatsApp\n+1 (778) 990-2716"
  },
  {
    "objectID": "Appendix.html#linkedin",
    "href": "Appendix.html#linkedin",
    "title": "Appendix",
    "section": "üåê LinkedIn",
    "text": "üåê LinkedIn\nlinkedin.com/in/Nelson"
  },
  {
    "objectID": "Appendix.html#download-resume",
    "href": "Appendix.html#download-resume",
    "title": "Appendix",
    "section": "üñáÔ∏è Download Resume",
    "text": "üñáÔ∏è Download Resume\nDownload Resume (PDF)\n\nFeel free to reach out regarding collaboration, project opportunities, or just to connect!\n\n\nLeadership & Community Involvement (Optional) (leadership.qmd) markdown Copy Edit ‚Äî title: ‚ÄúLeadership & Community‚Äù format: html ‚Äî"
  },
  {
    "objectID": "Appendix.html#project-assistant-role-cross-functional-leadership",
    "href": "Appendix.html#project-assistant-role-cross-functional-leadership",
    "title": "Appendix",
    "section": "üíº Project Assistant Role ‚Äì Cross-Functional Leadership",
    "text": "üíº Project Assistant Role ‚Äì Cross-Functional Leadership\n\nLiaised with marketing, operations, and logistics to streamline efforts\n\nChampioned collaborative tools to improve internal coordination"
  },
  {
    "objectID": "Appendix.html#community-engagement",
    "href": "Appendix.html#community-engagement",
    "title": "Appendix",
    "section": "üß° Community Engagement",
    "text": "üß° Community Engagement\n\nOrganized events for international student integration\n\nVolunteered for local food bank initiatives in Vancouver\n\n\n\n‚ÄúLeadership is not about a title or designation. It‚Äôs about impact, influence, and inspiration.‚Äù ‚Äì Robin Sharma"
  },
  {
    "objectID": "Appendix.html#breif-comment-on-portfolio-sources-and-ideas",
    "href": "Appendix.html#breif-comment-on-portfolio-sources-and-ideas",
    "title": "Appendix",
    "section": "Breif Comment on Portfolio sources and ideas",
    "text": "Breif Comment on Portfolio sources and ideas\nThe LLM-generated response provided a general and foundational structure for portfolio building. Drawing from it, I emphasized my education, skills, and experiences rooted in my personal and professional background. I also expanded the portfolio by including dedicated sections on Projects and Certifications, highlighting my expertise and ongoing skill development. These additions determine the depth of my experience and also position my portfolio as a strong and competitive tool in the job market, helping it stand out from the rest."
  },
  {
    "objectID": "Appendix.html#sample-one-page-mba-student-portfolio-text-only",
    "href": "Appendix.html#sample-one-page-mba-student-portfolio-text-only",
    "title": "Appendix",
    "section": "üìù Sample One-Page MBA Student Portfolio (Text Only)",
    "text": "üìù Sample One-Page MBA Student Portfolio (Text Only)\nNELSON JAMES MBA Candidate | Schulich School of Business, York University üìß nelson.james@email.com | üìû 647-555-1234 | linkedin.com/in/nelsonjames"
  },
  {
    "objectID": "Appendix.html#professional-summary",
    "href": "Appendix.html#professional-summary",
    "title": "Appendix",
    "section": "Professional Summary",
    "text": "Professional Summary\nResults-driven MBA candidate with 7+ years of experience in network security at Fortune 500 firms. Passionate about combining business acumen with cloud technology to deliver scalable, secure enterprise solutions."
  },
  {
    "objectID": "Appendix.html#education-1",
    "href": "Appendix.html#education-1",
    "title": "Appendix",
    "section": "Education",
    "text": "Education\nSchulich School of Business, York University ‚Äî Toronto, ON Master of Business Administration (MBA) | Expected: August 2025\nFocus: Technology Management & Strategy\nRelevant Coursework: Strategic Management, Cloud Computing, Data Analytics\nDean‚Äôs List (2024)"
  },
  {
    "objectID": "Appendix.html#key-skills",
    "href": "Appendix.html#key-skills",
    "title": "Appendix",
    "section": "Key Skills",
    "text": "Key Skills\nCloud Architecture ‚Ä¢ Cybersecurity Risk Management ‚Ä¢ Data Analysis (Excel, Tableau) Project Leadership ‚Ä¢ Financial Modeling ‚Ä¢ Stakeholder Communication ‚Ä¢ Agile & Scrum"
  },
  {
    "objectID": "Appendix.html#professional-experience",
    "href": "Appendix.html#professional-experience",
    "title": "Appendix",
    "section": "Professional Experience",
    "text": "Professional Experience\nSoftware Engineer ‚Äì Wipro Technologies, Toronto, ON | 2017‚Äì2023\nLed cybersecurity assessments for enterprise clients, reducing vulnerability risk by 35%.\nManaged cross-functional teams to implement security upgrades across 10+ global locations.\nDelivered client workshops on secure network practices, improving compliance scores by 20%."
  },
  {
    "objectID": "Appendix.html#leadership-certifications",
    "href": "Appendix.html#leadership-certifications",
    "title": "Appendix",
    "section": "Leadership & Certifications",
    "text": "Leadership & Certifications\nAWS Certified Solutions Architect ‚Äì Associate (2025)\nPresident, MBA Technology Club ‚Äì Organized panels with industry leaders and alumni\nTeam Lead, MBA Case Competition Finalist (Top 5 out of 120 teams)"
  },
  {
    "objectID": "Appendix.html#career-interests",
    "href": "Appendix.html#career-interests",
    "title": "Appendix",
    "section": "Career Interests",
    "text": "Career Interests\nEnterprise Cloud Security Architect | Digital Transformation Consulting | Technology Strategy"
  },
  {
    "objectID": "index.html#skills",
    "href": "index.html#skills",
    "title": "Shilpi Kumari Portfolio",
    "section": "üß† Skills",
    "text": "üß† Skills\nAI/ML & Analytics: Data visualization, data cleaning, feature engineering, Python (pandas, NumPy, scikit-learn, Matplotlib, Seaborn), ML Models (KNN, Logistic Regression), Generative AI, Business Intelligence, Networking: TCP/IP, LAN/WAN, Routing(BGP,OSPF,ISIS), MPLS, HSRP, VRRP, VPN, DNS, DHCP, Fabric, RADIUS Server, NAT, Ethernet, SSL, SSH Software : ServiceNow, Remedy, SIEM, Nagios, Solarwinds, Wireshark, Netwflow Analyzer, HPNMi, Vs code, HTML/CSS, Github, Citrix, Vmware, Visio Programming & Tools: Python, MySQL, Machine Learning, Git/GitHub, VS Code, Jupyter, scikit-learn, pandas, matplotlib, NumPy Cloud: EC2, Compute, AWS Lambda, VPC, IAM, Dynamodb, S3, Security, Auto-Scaling, Cloud front, CDN, API, CI/CD concepts, Infrastructure-as-Code (Terraform basics), Docker Soft Skills: interpersonal skills, problem-solving, problem identification, configuration management, technical support, Stakeholder collaboration, technical presentation, team facilitation, customer success mindset"
  },
  {
    "objectID": "index.html#certifications",
    "href": "index.html#certifications",
    "title": "Shilpi Kumari Portfolio",
    "section": "üìú Certifications",
    "text": "üìú Certifications\n\nCCNA | Cisco Certified Network Associate\n\nCisco Meraki Black Belt\n\nAWS Solutions Architect - Associate (In Progress)\nGoogle cloud networking certification (In Progress)"
  },
  {
    "objectID": "index.html#connect-with-me",
    "href": "index.html#connect-with-me",
    "title": "Shilpi Kumari Portfolio",
    "section": "üåê Connect with Me",
    "text": "üåê Connect with Me\n\nLinkedIn\nInstagram\n\n\nüí¨ Prefer a pop-up?\nClick the floating button in the bottom corner instead!"
  },
  {
    "objectID": "projects/Customer_Churn_Analysis_using_KNN_and_Logistic_Regression.html",
    "href": "projects/Customer_Churn_Analysis_using_KNN_and_Logistic_Regression.html",
    "title": "New Section",
    "section": "",
    "text": "# Loading and Exploring Social Media and Telecom Dataset\n## Explanation\nThis section connects to Google Drive and loads two datasets‚ÄîSocial Media and Telecom‚Äîinto pandas DataFrames. It allows for an initial exploration by printing the first few rows of each dataset, providing insight into the structure and types of data available.\n# Import necessary libraries\nimport pandas as pd\nfrom google.colab import drive\n\n# Mount Google Drive\ndrive.mount('/content/drive')\n\n# Defining file paths for both datasets\nsocialmedia_path = '/content/drive/MyDrive/socialmedia.csv'  # Update if in a specific folder\ntelecom_path = '/content/drive/MyDrive/telecom.csv'  # Update if in a specific folder\n\n# Load the social media dataset\nsocialmedia_df = pd.read_csv(socialmedia_path)\nprint(\"Social Media Dataset:\")\nprint(socialmedia_df.head())  # Display the first few rows\n\n# Load the telecom dataset\ntelecom_df = pd.read_csv(telecom_path)\nprint(\"\\nTelecom Dataset:\")\nprint(telecom_df.head())  # Display the first few rows\n\n\n\n\n\n\nMounted at /content/drive\nSocial Media Dataset:\n   Page total likes    Type  Category  Post Month  Post Weekday  Post Hour  \\\n0            139441   Photo         2          12             4          3   \n1            139441  Status         2          12             3         10   \n2            139441   Photo         3          12             3          3   \n3            139441   Photo         2          12             2         10   \n4            139441   Photo         2          12             2          3   \n\n   Paid  Lifetime Post Total Reach  Lifetime Post Total Impressions  \\\n0   0.0                       2752                             5091   \n1   0.0                      10460                            19057   \n2   0.0                       2413                             4373   \n3   1.0                      50128                            87991   \n4   0.0                       7244                            13594   \n\n   Lifetime Engaged Users  Lifetime Post Consumers  \\\n0                     178                      109   \n1                    1457                     1361   \n2                     177                      113   \n3                    2211                      790   \n4                     671                      410   \n\n   Lifetime Post Consumptions  \\\n0                         159   \n1                        1674   \n2                         154   \n3                        1119   \n4                         580   \n\n   Lifetime Post Impressions by people who have liked your Page  \\\n0                                               3078              \n1                                              11710              \n2                                               2812              \n3                                              61027              \n4                                               6228              \n\n   Lifetime Post reach by people who like your Page  \\\n0                                              1640   \n1                                              6112   \n2                                              1503   \n3                                             32048   \n4                                              3200   \n\n   Lifetime People who have liked your Page and engaged with your post  \\\n0                                                119                     \n1                                               1108                     \n2                                                132                     \n3                                               1386                     \n4                                                396                     \n\n   comment    like  share  Total Interactions  \n0        4    79.0   17.0                 100  \n1        5   130.0   29.0                 164  \n2        0    66.0   14.0                  80  \n3       58  1572.0  147.0                1777  \n4       19   325.0   49.0                 393  \n\nTelecom Dataset:\n   Call  Failure  Complains  Subscription  Length  Charge  Amount  \\\n0              8          0                    38               0   \n1              0          0                    39               0   \n2             10          0                    37               0   \n3             10          0                    38               0   \n4              3          0                    38               0   \n\n   Seconds of Use  Frequency of use  Frequency of SMS  \\\n0            4370                71                 5   \n1             318                 5                 7   \n2            2453                60               359   \n3            4198                66                 1   \n4            2393                58                 2   \n\n   Distinct Called Numbers  Age Group  Tariff Plan  Status  Age  \\\n0                       17          3            1       1   30   \n1                        4          2            1       2   25   \n2                       24          3            1       1   30   \n3                       35          1            1       1   15   \n4                       33          1            1       1   15   \n\n   Customer Value  Churn  \n0         197.640      0  \n1          46.035      0  \n2        1536.520      0  \n3         240.020      0  \n4         145.805      0"
  },
  {
    "objectID": "projects/Customer_Churn_Analysis_using_KNN_and_Logistic_Regression.html#explanation",
    "href": "projects/Customer_Churn_Analysis_using_KNN_and_Logistic_Regression.html#explanation",
    "title": "New Section",
    "section": "Explanation",
    "text": "Explanation\nThis section focuses on the data cleaning necessary for preparing the Social Media Metrics dataset for machine learning. It includes selecting relevant input features and the target variable, handling missing values, and previewing the cleaned dataset. These steps are essential before applying any modelling techniques.\n\n# Import necessary libraries\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n# Load the Social Media Metrics dataset\nsocialmedia_df = pd.read_csv('/content/drive/MyDrive/socialmedia.csv')\n\n# Selecting relevant features and target variable\nfeatures = ['Page total likes', 'Type', 'Category', 'Post Hour', 'Paid']\nX_socialmedia = socialmedia_df[features]\ny_socialmedia = socialmedia_df['Lifetime Engaged Users']\n\n# Handling missing values\nX_socialmedia = X_socialmedia.fillna(0)  # Replace with more appropriate imputation if needed\n\n# Display dataset structure\nprint(\"Features dataset:\")\nprint(X_socialmedia.head())\nprint(\"\\nTarget variable:\")\nprint(y_socialmedia.head())"
  },
  {
    "objectID": "projects/Customer_Churn_Analysis_using_KNN_and_Logistic_Regression.html#explanation-1",
    "href": "projects/Customer_Churn_Analysis_using_KNN_and_Logistic_Regression.html#explanation-1",
    "title": "New Section",
    "section": "Explanation",
    "text": "Explanation\nThis section defines the preprocessing steps and creates a machine learning pipeline for predicting user engagement (Lifetime Engaged Users) using a linear regression model. It also prepares the data for training and testing by splitting it into subsets.\n\n# Define the preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), ['Page total likes', 'Post Hour', 'Paid']),\n        ('cat', OneHotEncoder(handle_unknown='ignore'), ['Type', 'Category'])\n    ])\n\n# Define the pipeline\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('regressor', LinearRegression())\n])\n\n# Split the data into training and testing sets (80-20 split)\nX_train_sm, X_test_sm, y_train_sm, y_test_sm = train_test_split(X_socialmedia, y_socialmedia, test_size=0.2, random_state=42)"
  },
  {
    "objectID": "projects/Customer_Churn_Analysis_using_KNN_and_Logistic_Regression.html#explanation-2",
    "href": "projects/Customer_Churn_Analysis_using_KNN_and_Logistic_Regression.html#explanation-2",
    "title": "New Section",
    "section": "Explanation",
    "text": "Explanation\nThis section completes the regression modeling process by training the pipeline, generating predictions, and evaluating performance on both training and test datasets. This helps assess how well the model generalizes to new, unseen data.\n\n# Fit the model on training data\npipeline.fit(X_train_sm, y_train_sm)\n\n# Make predictions on both training and testing data\ny_train_pred_sm = pipeline.predict(X_train_sm)\ny_test_pred_sm = pipeline.predict(X_test_sm)\n\n# Evaluate the model\ntrain_mse_sm = mean_squared_error(y_train_sm, y_train_pred_sm)\ntest_mse_sm = mean_squared_error(y_test_sm, y_test_pred_sm)\ntrain_r2_sm = r2_score(y_train_sm, y_train_pred_sm)\ntest_r2_sm = r2_score(y_test_sm, y_test_pred_sm)\n\nprint(f\"Training MSE: {train_mse_sm}, Training R¬≤: {train_r2_sm}\")\nprint(f\"Testing MSE: {test_mse_sm}, Testing R¬≤: {test_r2_sm}\")\n\nTraining MSE: 851154.5821688741, Training R¬≤: 0.20570089686390247\nTesting MSE: 459291.161656748, Testing R¬≤: 0.1558370513520827"
  },
  {
    "objectID": "projects/Customer_Churn_Analysis_using_KNN_and_Logistic_Regression.html#explanation-3",
    "href": "projects/Customer_Churn_Analysis_using_KNN_and_Logistic_Regression.html#explanation-3",
    "title": "New Section",
    "section": "Explanation",
    "text": "Explanation\nThis section extracts and displays the importance of each feature in the regression model by analyzing the coefficients assigned during training. It provides insight into which variables most significantly affect predicted user engagement.\n\n# Get feature names after encoding\nfeature_names = preprocessor.named_transformers_['cat'].get_feature_names_out(['Type', 'Category'])\nall_feature_names = ['Page total likes', 'Post Hour', 'Paid'] + list(feature_names)\n\n# Model coefficients\ncoefficients = pipeline.named_steps['regressor'].coef_\n\n# Display feature importance\nfeature_importance = pd.DataFrame({'Feature': all_feature_names, 'Coefficient': coefficients})\nfeature_importance = feature_importance.sort_values(by='Coefficient', key=abs, ascending=False)\nprint(\"\\nFeature Importance (sorted by absolute coefficient values):\")\nprint(feature_importance)\n\n\n\nFeature Importance (sorted by absolute coefficient values):\n            Feature  Coefficient\n3         Type_Link -1131.852370\n5       Type_Status   994.154893\n6        Type_Video   652.484424\n4        Type_Photo  -514.786947\n0  Page total likes  -231.231917\n2              Paid   132.694985\n7        Category_1    63.564900\n1         Post Hour   -56.536562\n8        Category_2   -33.876303\n9        Category_3   -29.688597"
  },
  {
    "objectID": "projects/Customer_Churn_Analysis_using_KNN_and_Logistic_Regression.html#explanation-4",
    "href": "projects/Customer_Churn_Analysis_using_KNN_and_Logistic_Regression.html#explanation-4",
    "title": "New Section",
    "section": "Explanation",
    "text": "Explanation\nThis section loads the Telecom Churn dataset and performs an initial data inspection to prepare for churn prediction. It identifies the target variable (Churn) and separates it from the feature set (X_telecom), setting the stage for future preprocessing and model building.\n\nimport pandas as pd\n\n# Load the churn dataset (make sure the path is correct)\ntelecom_df = pd.read_csv('/content/drive/MyDrive/telecom.csv')\n\n# Check the first few rows of the dataset to confirm it loaded correctly\nprint(\"Dataset Sample:\")\nprint(telecom_df.head())\n\n# Define the target variable 'Churn' and separate features\nX_telecom = telecom_df.drop(columns=['Churn'], errors='ignore')\ny_telecom = telecom_df['Churn']\n\n# Verify column names and structure of the data\nprint(\"\\nFeature Columns:\")\nprint(X_telecom.columns)\nprint(\"\\nTarget Variable Sample:\")\nprint(y_telecom.head())\n\nDataset Sample:\n   Call  Failure  Complains  Subscription  Length  Charge  Amount  \\\n0              8          0                    38               0   \n1              0          0                    39               0   \n2             10          0                    37               0   \n3             10          0                    38               0   \n4              3          0                    38               0   \n\n   Seconds of Use  Frequency of use  Frequency of SMS  \\\n0            4370                71                 5   \n1             318                 5                 7   \n2            2453                60               359   \n3            4198                66                 1   \n4            2393                58                 2   \n\n   Distinct Called Numbers  Age Group  Tariff Plan  Status  Age  \\\n0                       17          3            1       1   30   \n1                        4          2            1       2   25   \n2                       24          3            1       1   30   \n3                       35          1            1       1   15   \n4                       33          1            1       1   15   \n\n   Customer Value  Churn  \n0         197.640      0  \n1          46.035      0  \n2        1536.520      0  \n3         240.020      0  \n4         145.805      0  \n\nFeature Columns:\nIndex(['Call  Failure', 'Complains', 'Subscription  Length', 'Charge  Amount',\n       'Seconds of Use', 'Frequency of use', 'Frequency of SMS',\n       'Distinct Called Numbers', 'Age Group', 'Tariff Plan', 'Status', 'Age',\n       'Customer Value'],\n      dtype='object')\n\nTarget Variable Sample:\n0    0\n1    0\n2    0\n3    0\n4    0\nName: Churn, dtype: int64"
  },
  {
    "objectID": "projects/Customer_Churn_Analysis_using_KNN_and_Logistic_Regression.html#explanation-5",
    "href": "projects/Customer_Churn_Analysis_using_KNN_and_Logistic_Regression.html#explanation-5",
    "title": "New Section",
    "section": "Explanation",
    "text": "Explanation\n\nData Preparation and Cleaning\nThe telecom dataset was loaded from Google Drive and inspected for missing values.\nMissing data was handled by dropping rows with null entries to ensure the dataset was clean and complete before modeling.\n\n\nFeature Selection and Separation\nThe target variable is ‚ÄòChurn‚Äô, indicating whether a customer left the service.\nAll remaining columns were treated as input features, further divided into:\nNumerical features (e.g., age, usage metrics)\nCategorical features (Age Group, Tariff Plan, Status)\n\n\nPreprocessing Pipeline\nTo prepare the data for modeling:\nNumerical features were standardized using StandardScaler to ensure all values are on a similar scale.\nCategorical features were encoded using OneHotEncoder, converting categories into a machine-readable numeric format.\nA ColumnTransformer combined these preprocessing steps, and each model was embedded in a pipeline to ensure consistent processing.\n\n\nModel Training and Evaluation\nThe dataset was split into training (80%) and testing (20%) sets.\n\nTwo models were trained:\nLogistic Regression: A linear classifier suitable for binary classification and interpreting feature importance.\nK-Nearest Neighbors (KNN): A distance-based classifier that predicts labels based on the nearest data points in the feature space.\nEach model was evaluated using accuracy score on the test set, providing an initial measure of predictive performance.\n\n\n\nHyperparameter Tuning for KNN\nSince KNN‚Äôs performance depends heavily on the choice of K (number of neighbors), a loop tested values of K from 1 to 20.\nThe accuracy for each K was recorded and visualized using a line plot.\nThe optimal K was identified ‚Äî the one that achieved the highest test accuracy.\n\n\nFeature Importance Analysis (Logistic Regression)\nCoefficients from the logistic regression model were extracted to assess the impact of each feature on churn.\nPositive coefficients indicate features that increase the likelihood of churn.\nNegative coefficients indicate features that decrease the likelihood.\nThis insight helps stakeholders understand which variables are most critical in predicting customer behavior.\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n\n# Load the dataset\ntelecom_path = '/content/drive/MyDrive/telecom.csv'  # Update if in a specific folder\ntelecom_df = pd.read_csv(telecom_path)\n\n\n# Check for missing values\nprint(\"Missing Values:\", telecom_df.isnull().sum())\n\n# Handle missing values (for simplicity, we'll drop rows with missing values)\ntelecom_df = telecom_df.dropna()\n\n# Define feature columns and target variable\nX_telecom = telecom_df.drop('Churn', axis=1)  # Features\ny_telecom = telecom_df['Churn']  # Target\n\n# Separate categorical and numerical features\ncategorical_features = ['Age Group', 'Tariff Plan', 'Status']  # Example of categorical features\nnumerical_features = X_telecom.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\n# Preprocessing pipeline for numerical and categorical features\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', StandardScaler(), numerical_features),  # Scale numerical features\n        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)  # Encode categorical features\n    ])\n\n# Create a pipeline for Logistic Regression and KNN models\nlogreg_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', LogisticRegression())\n])\n\nknn_pipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', KNeighborsClassifier())\n])\n\n# Split data into training and testing sets (80-20 split)\nX_train, X_test, y_train, y_test = train_test_split(X_telecom, y_telecom, test_size=0.2, random_state=42)\n\n# Train Logistic Regression model\nlogreg_pipeline.fit(X_train, y_train)\n\n# Train KNN model (with K=5)\nknn_pipeline.fit(X_train, y_train)\n\n# Predictions\nlogreg_preds = logreg_pipeline.predict(X_test)\nknn_preds = knn_pipeline.predict(X_test)\n\n# Evaluate models using accuracy\nlogreg_accuracy = accuracy_score(y_test, logreg_preds)\nknn_accuracy = accuracy_score(y_test, knn_preds)\n\nprint(f\"Logistic Regression Accuracy: {logreg_accuracy}\")\nprint(f\"KNN Accuracy (K=5): {knn_accuracy}\")\n\n# Explore effect of changing K in KNN\nk_values = range(1, 21)  # Test K values from 1 to 20\nknn_accuracies = []\n\nfor k in k_values:\n    knn_pipeline.set_params(classifier=KNeighborsClassifier(n_neighbors=k))\n    knn_pipeline.fit(X_train, y_train)\n    knn_preds = knn_pipeline.predict(X_test)\n    knn_accuracies.append(accuracy_score(y_test, knn_preds))\n\n# Plot accuracy vs K\nplt.plot(k_values, knn_accuracies, marker='o')\nplt.title('KNN Accuracy vs K')\nplt.xlabel('K (Number of Neighbors)')\nplt.ylabel('Accuracy')\nplt.show()\n\n# Best K\nbest_k = k_values[knn_accuracies.index(max(knn_accuracies))]\nprint(f\"Best K for KNN: {best_k}\")\n\n# Analysis: Most important features based on model coefficients (Logistic Regression)\nlogreg_coef = logreg_pipeline.named_steps['classifier'].coef_[0]\nfeature_names = numerical_features + list(preprocessor.transformers_[1][1].get_feature_names_out(categorical_features))\nfeature_importance = pd.DataFrame({'Feature': feature_names, 'Coefficient': logreg_coef})\nprint(feature_importance.sort_values(by='Coefficient', ascending=False))\n\nMissing Values: Call  Failure              0\nComplains                  0\nSubscription  Length       0\nCharge  Amount             0\nSeconds of Use             0\nFrequency of use           0\nFrequency of SMS           0\nDistinct Called Numbers    0\nAge Group                  0\nTariff Plan                0\nStatus                     0\nAge                        0\nCustomer Value             0\nChurn                      0\ndtype: int64\nLogistic Regression Accuracy: 0.8698412698412699\nKNN Accuracy (K=5): 0.9317460317460318\n\n\n\n\n\n\n\n\n\nBest K for KNN: 2\n                    Feature  Coefficient\n1                 Complains     1.103923\n0             Call  Failure     0.868096\n4            Seconds of Use     0.865874\n14              Age Group_2     0.695437\n16              Age Group_4     0.675648\n15              Age Group_3     0.533240\n10                   Status     0.351586\n12           Customer Value     0.192794\n8                 Age Group     0.149933\n21                 Status_2     0.148200\n9               Tariff Plan     0.137933\n19            Tariff Plan_2     0.036391\n18            Tariff Plan_1    -0.046620\n11                      Age    -0.154419\n20                 Status_1    -0.158429\n2      Subscription  Length    -0.175953\n7   Distinct Called Numbers    -0.365365\n3            Charge  Amount    -0.629595\n17              Age Group_5    -0.918529\n13              Age Group_1    -0.996025\n6          Frequency of SMS    -1.728051\n5          Frequency of use    -2.192656"
  },
  {
    "objectID": "projects/Customer_Churn_Analysis_using_KNN_and_Logistic_Regression.html#model-performance-summary",
    "href": "projects/Customer_Churn_Analysis_using_KNN_and_Logistic_Regression.html#model-performance-summary",
    "title": "New Section",
    "section": "Model Performance Summary",
    "text": "Model Performance Summary\nAfter preprocessing and training both models on the telecom dataset, the following results were observed:\n\nLogistic Regression Accuracy: 86.98%\n\n\nK-Nearest Neighbors (K=5) Accuracy: 93.17%\nOptimal K for KNN: 2 yielded the highest accuracy among the tested values (1‚Äì20).\nThese results indicate that KNN outperformed Logistic Regression in terms of predictive accuracy, making it the stronger model for identifying churn risk in this dataset."
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "üìÑ Download My Resume (PDF)\n\n\nExperienced Network Administrator with over 7 years of experience managing enterprise LAN/WAN operations and multi-vendor device configurations. Demonstrated expertise in implementing robust firewall policies and conducting vulnerability assessments to enhance network security. Successfully collaborated on fibre network projects and optimized workflows for improved operational efficiency. Eager to leverage comprehensive technical skills in routing, switching, and cloud networking to support and advance IT infrastructure environments.\n\n\n\n\n\n\nDesigned and deployed scalable cloud solutions across AWS for enterprise clients in telecom and retail,resulting in 20% improvement in application performance/cost savings.\nManaged hybrid client network solutions of multi-vendor environments(Cisco, Juniper, FortiGate, F5),ensuring 95% network uptime and resolving critical issues within critical 4 hours timeframe.‚Äù\nMentored new hires, managed multi-client SLAs ,consistently achieving 99% SLA adherence and improving team efficiency by 20%.‚Äù\n\n\n\n\n\nPartnered with cross-functional teams to conduct thorough discovery and implementation of fibre networks to sites,successfully connecting 15 number of sites within 2 months timeframe.‚Äù\nIdentified and resolved process issues to drive optimal workflow and business growth,resulting in 15% improvement in process efficiency/cost savings.\nDeveloped SOPs and resolved telecom operations ,reducing issue resolution time by 25%.‚Äù\n\n\n\n\n\nManaged LAN/WAN infrastructure and executed scheduled patching of routers, firewalls, and switches, reducing security vulnerabilities by 10% and ensuring 95% system uptime.‚Äù\nConfigure and onbarded new switches and router on clinet demands for new site extensions and support,completing an average of 2 new site setups per month/quarter.‚Äù\n\n\n\n\n\nMBA, University Canada West\nExpected: 2025\nBachelor‚Äôs Degree, [Bachelors of Computer Application], [Patna University]\nGraduated: [2015]\n\n\n\nCloud & Network: AWS, EC2, Lambda, Cisco Switches & Routers, Juniper, Meraki, Firewalls (Fortigate & Palo-Alto), Cisco Meraki , Data-Centre (Nexus switches 5500,7700).\nTools & Tech: Python, SQL, Tableau, Linux , ML (for data analysis)\nSoft Skills: Project Management, Communication, Problem Solving , Multi-tasking\n\n\n\n\nCCNA | Cisco Certified Network Associate\n\nCisco Meraki Black Belt\n\nAWS Cloud Practitioner\nDigital Marketing\n\n\n\n\n\nEnglish (ADVANCED)\nHINDI (Proficient)\n\n\n\n\n-Volunteer Coordinator, Local Community Center - Assisted with event planning and community outreach to support local engagement."
  },
  {
    "objectID": "resume.html#professional-summary",
    "href": "resume.html#professional-summary",
    "title": "Resume",
    "section": "",
    "text": "Experienced Network Administrator with over 7 years of experience managing enterprise LAN/WAN operations and multi-vendor device configurations. Demonstrated expertise in implementing robust firewall policies and conducting vulnerability assessments to enhance network security. Successfully collaborated on fibre network projects and optimized workflows for improved operational efficiency. Eager to leverage comprehensive technical skills in routing, switching, and cloud networking to support and advance IT infrastructure environments."
  },
  {
    "objectID": "resume.html#experience",
    "href": "resume.html#experience",
    "title": "Resume",
    "section": "",
    "text": "Designed and deployed scalable cloud solutions across AWS for enterprise clients in telecom and retail,resulting in 20% improvement in application performance/cost savings.\nManaged hybrid client network solutions of multi-vendor environments(Cisco, Juniper, FortiGate, F5),ensuring 95% network uptime and resolving critical issues within critical 4 hours timeframe.‚Äù\nMentored new hires, managed multi-client SLAs ,consistently achieving 99% SLA adherence and improving team efficiency by 20%.‚Äù\n\n\n\n\n\nPartnered with cross-functional teams to conduct thorough discovery and implementation of fibre networks to sites,successfully connecting 15 number of sites within 2 months timeframe.‚Äù\nIdentified and resolved process issues to drive optimal workflow and business growth,resulting in 15% improvement in process efficiency/cost savings.\nDeveloped SOPs and resolved telecom operations ,reducing issue resolution time by 25%.‚Äù\n\n\n\n\n\nManaged LAN/WAN infrastructure and executed scheduled patching of routers, firewalls, and switches, reducing security vulnerabilities by 10% and ensuring 95% system uptime.‚Äù\nConfigure and onbarded new switches and router on clinet demands for new site extensions and support,completing an average of 2 new site setups per month/quarter.‚Äù"
  },
  {
    "objectID": "resume.html#education",
    "href": "resume.html#education",
    "title": "Resume",
    "section": "",
    "text": "MBA, University Canada West\nExpected: 2025\nBachelor‚Äôs Degree, [Bachelors of Computer Application], [Patna University]\nGraduated: [2015]"
  },
  {
    "objectID": "resume.html#skills",
    "href": "resume.html#skills",
    "title": "Resume",
    "section": "",
    "text": "Cloud & Network: AWS, EC2, Lambda, Cisco Switches & Routers, Juniper, Meraki, Firewalls (Fortigate & Palo-Alto), Cisco Meraki , Data-Centre (Nexus switches 5500,7700).\nTools & Tech: Python, SQL, Tableau, Linux , ML (for data analysis)\nSoft Skills: Project Management, Communication, Problem Solving , Multi-tasking"
  },
  {
    "objectID": "resume.html#certifications",
    "href": "resume.html#certifications",
    "title": "Resume",
    "section": "",
    "text": "CCNA | Cisco Certified Network Associate\n\nCisco Meraki Black Belt\n\nAWS Cloud Practitioner\nDigital Marketing"
  },
  {
    "objectID": "resume.html#languages",
    "href": "resume.html#languages",
    "title": "Resume",
    "section": "",
    "text": "English (ADVANCED)\nHINDI (Proficient)"
  },
  {
    "objectID": "resume.html#volunteer-experience",
    "href": "resume.html#volunteer-experience",
    "title": "Resume",
    "section": "",
    "text": "-Volunteer Coordinator, Local Community Center - Assisted with event planning and community outreach to support local engagement."
  },
  {
    "objectID": "sss.html",
    "href": "sss.html",
    "title": "Resume | Shilpi Kumari",
    "section": "",
    "text": "Hi, I‚Äôm Shilpi Kumari\n\n\nCloud Engineer | Network Analyst | MBA Candidate\n\n\n\n\n\nüéì Education\n\n\n\nMBA ‚Äì University Canada West (Expected June 2025)\n\n\nBCA ‚Äì Patna University\n\n\n\n\n\n\nüß† Skills\n\n\n&lt;div&gt;\n  &lt;h3&gt;Cloud & Networking&lt;/h3&gt;\n  &lt;ul&gt;\n    &lt;li&gt;AWS, EC2, Lambda&lt;/li&gt;\n    &lt;li&gt;Cisco, Juniper, Meraki&lt;/li&gt;\n    &lt;li&gt;Switching & Routing (OSPF, BGP, EIGRP)&lt;/li&gt;\n    &lt;li&gt;Firewalls: Fortigate, Palo-Alto&lt;/li&gt;\n    &lt;li&gt;Nexus Switches 5500/7700&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/div&gt;\n&lt;div&gt;\n  &lt;h3&gt;Tools & Soft Skills&lt;/h3&gt;\n  &lt;ul&gt;\n    &lt;li&gt;Python, SQL, Linux, Tableau, ML&lt;/li&gt;\n    &lt;li&gt;Project Management&lt;/li&gt;\n    &lt;li&gt;Problem Solving & Communication&lt;/li&gt;\n    &lt;li&gt;Multitasking & Team Collaboration&lt;/li&gt;\n  &lt;/ul&gt;\n&lt;/div&gt;\n\n\n\n\n\nüìú Certifications\n\n\n\nCCNA ‚Äì Cisco Certified Network Associate\n\n\nCisco Meraki Black Belt\n\n\nAWS Cloud Practitioner\n\n\nDMI ‚Äì Digital Marketing Institute\n\n\n\n\n\nüìÑ Download Resume (PDF)\n\n\n\n\nConnect: LinkedIn | Instagram\n\n\n¬© 2025 Shilpi Kumari | All Rights Reserved"
  },
  {
    "objectID": "Contact.html",
    "href": "Contact.html",
    "title": "Contact Me",
    "section": "",
    "text": "I‚Äôd love to hear from you! Whether it‚Äôs a question, collaboration opportunity, or feedback ‚Äî feel free to reach out through any of the channels below.\n\n\n\n+1 (778) 990-2716\n\n\n\n\nkr.shilpi21@gmail.com\n\n\n\n\nConnect with me on LinkedIn\n\n\n\n\nVisit my Portfolio\n\n\n\n\n20686 , Eastleigh Crescent , Langley , V3A0M4 , BC.\n\n\n\n\n\n Your Email:  \n Your Message: \n\n\nSend\n\n\n\nüîí Your privacy is important. Your contact info will never be shared."
  },
  {
    "objectID": "Contact.html#phone-whatsapp",
    "href": "Contact.html#phone-whatsapp",
    "title": "Contact Me",
    "section": "",
    "text": "+1 (778) 990-2716"
  },
  {
    "objectID": "Contact.html#email",
    "href": "Contact.html#email",
    "title": "Contact Me",
    "section": "",
    "text": "kr.shilpi21@gmail.com"
  },
  {
    "objectID": "Contact.html#linkedin",
    "href": "Contact.html#linkedin",
    "title": "Contact Me",
    "section": "",
    "text": "Connect with me on LinkedIn"
  },
  {
    "objectID": "Contact.html#portfolio",
    "href": "Contact.html#portfolio",
    "title": "Contact Me",
    "section": "",
    "text": "Visit my Portfolio"
  },
  {
    "objectID": "Contact.html#location",
    "href": "Contact.html#location",
    "title": "Contact Me",
    "section": "",
    "text": "20686 , Eastleigh Crescent , Langley , V3A0M4 , BC."
  },
  {
    "objectID": "Contact.html#contact-me-1",
    "href": "Contact.html#contact-me-1",
    "title": "Contact Me",
    "section": "",
    "text": "Your Email:  \n Your Message: \n\n\nSend\n\n\n\nüîí Your privacy is important. Your contact info will never be shared."
  },
  {
    "objectID": "Reflection.html",
    "href": "Reflection.html",
    "title": "My Portfolio",
    "section": "",
    "text": "Annoation on LLM Output\nAppendix - https://shilpi0786.github.io/Shilpi0786/Appendix.html\nI have ensured authenticity and relevance in the annotation process. I carefully reviewed the LLM outline, got the basic idea behind the concept, and annotated it based on personalization, skills, projects, aligning it with my actual academic and professional experience. I have used drafts as structured starting points and made generic revisions-\npersonalization - I have updated my professional experience and added real-time projects I have been working on to highlight my skills and experiences. I have listed projects such as AWS static site deployments, customer churn analysis, and work on under academic coursework.I have added other business projects, such as POpIN and an unsweetened organic marketing project, to highlight my skills on real-time projects. I have worked with different clients.\nContextual Editing - Some of the LLM phrases were edited for clarity and professionalism to simplify my tone. I have tried to edit in formal and more authentic, engaging phrases.\nVisual and Structural Changes - I have added sections like skills with different highlights of my technical and soft skills.I have removed unauthentic certifications listed in examples and only placed the authentic certifications completed by me.I have also included a logo of my own design created using Canva for maki ng it look visually appealing. Also, I have styled the CSS page using the styling formatting and dark-themed style for a bold look.\nAccuracy Checks- I have only included the experience, skills or certification which I hold and haven‚Äôt included LLM-generated inputs.\n\n\nReflection\nThroughout the process of developing my MBA portfolio website, my use of large language models (LLMs) such as ChatGPT evolved from simple content generation to a more conceptual format for the portfolio. Initially, I relied on the model to brainstorm ideas and draft text for common pages like the ‚ÄúAbout‚Äù and ‚ÄúProjects‚Äù sections.\nThese early drafts were useful for establishing a clear structure and format, but they often included basic content, generic phrasing, and assumptions that needed to be changed. The project progressed, and I shifted from using the LLM as a content creator to treating it more like a web designer, developing a website and hosting portfolio on GitHub. I began prompting the model for specific tasks, such as rephrasing bullet points to make them more concise, rewriting project styles using CSS to make them more modern and visually appealing, and trying to fix issues and errors I faced in configuring the styles in markdown. This iterative approach allowed me to maintain full control over the authenticity of my content while benefiting from the speed and fluency of the model‚Äôs outputs.\nOne of the most valuable aspects of using an LLM was the ability to brainstorm alternate layouts or formatting ideas and redesign my entire portfolio on how I want it to be crafted. For example, I was able to insert contact forms, logos, and my project links in different formats for my ‚ÄúProjects & Case Studies‚Äù section, categorized in technical and business sections, by giving a brief description and listing the tools used to make it look more engaging. This allowed me to experiment with usability and visual hierarchy while staying focused on the message I wanted to communicate.\nEnsuring accuracy was a critical part of the process. I fact-checked every data, tool, date, and description provided by the model and replaced any generic placeholders with details from my actual experiences, such as the PopIn Host Strategy project and the Unsweetened Juice Launch Plan and more real-life projects. I also annotated the LLM-generated drafts to indicate where changes were made for transparency, as required in the appendix.\nThis experience deepened my understanding of how to ethically and effectively collaborate with AI tools. Rather than passively accepting output, I engaged critically with it‚Äîfiltering, refining, and aligning it with my voice, values, and academic requirements. Ultimately, the integration of LLMs enhanced both the quality and efficiency of my portfolio, while allowing me to focus on meaningful personalization and presentation."
  },
  {
    "objectID": "Projects.html",
    "href": "Projects.html",
    "title": "Projects",
    "section": "",
    "text": "Course: Organizational Diagnosis\nTools Used: PESTELE, SWOT, Stakeholder Analysis Outcome: Proposed a strategic framework to enhance host engagement and brand visibility.\n\n\n\nRole: Project Manager\nWork: Built a Work Breakdown Structure and timeline for a cold-pressed organic juice launch.\nResult: Proposed cross-channel strategies including events, digital ads, and loyalty programs."
  },
  {
    "objectID": "Projects.html#popin-event-host-collaboration-strategy",
    "href": "Projects.html#popin-event-host-collaboration-strategy",
    "title": "Projects",
    "section": "",
    "text": "Course: Organizational Diagnosis\nTools Used: PESTELE, SWOT, Stakeholder Analysis Outcome: Proposed a strategic framework to enhance host engagement and brand visibility."
  },
  {
    "objectID": "Projects.html#unsweetened-juice-product-launch",
    "href": "Projects.html#unsweetened-juice-product-launch",
    "title": "Projects",
    "section": "",
    "text": "Role: Project Manager\nWork: Built a Work Breakdown Structure and timeline for a cold-pressed organic juice launch.\nResult: Proposed cross-channel strategies including events, digital ads, and loyalty programs."
  },
  {
    "objectID": "Projects.html#technical-projects",
    "href": "Projects.html#technical-projects",
    "title": "Projects",
    "section": "",
    "text": "Description: A serverless personal resume website deployed on AWS using S3 for hosting, Lambda for potential dynamic features, API Gateway for external interactions, and DynamoDB for data storage.\nTools: Deployment using AWS S3, Lambda, API Gateway, DynamoDB\n\nDetailed explanation in this file * link\n\n\n\n\nDescription: An AWS infrastructure setup for hosting a tour planning web application, leveraging EC2 for compute, RDS for database management, VPC for network isolation, and Security Groups for access control.\nTools: EC2, RDS, VPC, Security Groups\n\nDetailed explanation of configuation and deployment attached in file -\nPart 1\nPart 2\n\n\n\n\nA data analysis project using Python and machine learning models (KNN and Logistic Regression) to analyze customer data and predict churn probability.\nTools: Google Colab, Python, Scikit-learn (KNN, Logistic Regression), Pandas, NumPy"
  },
  {
    "objectID": "Skills.html",
    "href": "Skills.html",
    "title": "Skills & Certifications",
    "section": "",
    "text": "Cloud Platforms: AWS (EC2, Lambda), Cisco Meraki\nNetworking Devices: Cisco Switches & Routers, Juniper, Nexus Switches (5500, 7700)\nSwitching Concepts: VLANs, STP (Spanning Tree Protocol), EtherChannel, VTP\nRouting Protocols: OSPF, EIGRP, BGP, Static Routing\nFirewall Technologies\n\nFortiGate (policy-based & profile-based rules)\nPalo Alto (App-ID, NAT, Zone-based security)\nCisco ASA & Meraki Firewalls\n\n\n\n\n\n\nProgramming: Python\nDatabase: SQL\nData Visualization: Tableau\nOperating Systems: Linux (Bash, system administration)\nMachine Learning: Applied ML for data analysis"
  },
  {
    "objectID": "Skills.html#cloud-networking",
    "href": "Skills.html#cloud-networking",
    "title": "Skills & Certifications",
    "section": "",
    "text": "Cloud Platforms: AWS (EC2, Lambda), Cisco Meraki\nNetworking Devices: Cisco Switches & Routers, Juniper, Nexus Switches (5500, 7700)\nSwitching Concepts: VLANs, STP (Spanning Tree Protocol), EtherChannel, VTP\nRouting Protocols: OSPF, EIGRP, BGP, Static Routing\nFirewall Technologies\n\nFortiGate (policy-based & profile-based rules)\nPalo Alto (App-ID, NAT, Zone-based security)\nCisco ASA & Meraki Firewalls"
  },
  {
    "objectID": "Skills.html#tools-technologies",
    "href": "Skills.html#tools-technologies",
    "title": "Skills & Certifications",
    "section": "",
    "text": "Programming: Python\nDatabase: SQL\nData Visualization: Tableau\nOperating Systems: Linux (Bash, system administration)\nMachine Learning: Applied ML for data analysis"
  },
  {
    "objectID": "python project.html",
    "href": "python project.html",
    "title": "Python Project",
    "section": "",
    "text": "title: ‚ÄúPython Project‚Äù format: html"
  },
  {
    "objectID": "python project.html#data-anlaysis",
    "href": "python project.html#data-anlaysis",
    "title": "Python Project",
    "section": "data anlaysis",
    "text": "data anlaysis"
  },
  {
    "objectID": "Projects.html#project-overview",
    "href": "Projects.html#project-overview",
    "title": "Projects",
    "section": "üõ°Ô∏è Project Overview",
    "text": "üõ°Ô∏è Project Overview\nA machine learning pipeline developed in Google Colab to classify network traffic as ‚ÄòSafe‚Äô or ‚ÄòMalicious‚Äô. By analyzing behavioral features like authentication failures and CPU spikes, the model serves as a proactive defense mechanism against cyber threats."
  },
  {
    "objectID": "Projects.html#key-technical-steps",
    "href": "Projects.html#key-technical-steps",
    "title": "Projects",
    "section": "üõ†Ô∏è Key Technical Steps",
    "text": "üõ†Ô∏è Key Technical Steps\n\nExploratory Data Analysis (EDA): Visualized feature correlations using Pearson coefficients to identify primary threat indicators.\nPreprocessing: Implemented StandardScaler for feature normalization and used stratified splitting to handle class distribution.\nSVM Kernel Analysis: Evaluated Linear, RBF, and Polynomial kernels to find the optimal decision boundary.\nProbabilistic Baseline: Developed a Logistic Regression model to compare distance-based classification with probabilistic outputs."
  },
  {
    "objectID": "Projects.html#final-results",
    "href": "Projects.html#final-results",
    "title": "Projects",
    "section": "üìä Final Results",
    "text": "üìä Final Results\nThe Linear SVM outperformed other models in a balanced security context.\n\n\n\nModel\nAccuracy\nRecall (Detection Rate)\nPrecision\nROC-AUC\n\n\n\n\nSVM Linear\n93%\n92.3%\n90.0%\n0.981\n\n\nLogistic Regression\n92%\n92.3%\n87.8%\n0.980\n\n\n\n\nKey Insight:\nThe high ROC-AUC of 0.981 suggests that the selected network features are highly discriminative. While the Polynomial kernel offered 100% precision, its lower recall makes it less suitable for high-security environments where missing an attack is a high-risk failure.\nTools: Google Colab, Python, Scikit-learn (SVM, Logistic Regression), Seaborn, Pandas\n\n[Preview] https://colab.research.google.com/drive/1NmCZhni8qfvlThuH8joMFvcI3iolVX-0?usp=sharing\n\n\n\nüü¢ Static Resume Website on AWS\nDescription: A serverless personal resume website deployed on AWS using S3 for hosting, Lambda for potential dynamic features, API Gateway for external interactions, and DynamoDB for data storage.\nTools: Deployment using AWS S3, Lambda, API Gateway, DynamoDB\n\nDetailed explanation in this file * link\n\n\n\nüü¢ AWS Web Server Hosting for Tour Planning\nDescription: An AWS infrastructure setup for hosting a tour planning web application, leveraging EC2 for compute, RDS for database management, VPC for network isolation, and Security Groups for access control.\nTools: EC2, RDS, VPC, Security Groups\n\nDetailed explanation of configuation and deployment attached in file -\nPart 1\nPart 2\n\n\n\nüü¢ Customer Churn Analysis Using KNN and Logistic Regressions\nA data analysis project using Python and machine learning models (KNN and Logistic Regression) to analyze customer data and predict churn probability.\nTools: Google Colab, Python, Scikit-learn (KNN, Logistic Regression), Pandas, NumPy\n\nPreview"
  }
]